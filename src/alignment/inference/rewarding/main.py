#!/usr/bin/env python
# coding: utf-8

"""
Input file: `prompt_responses.json`
This file aims to assign reward for the responses of each prompt, and then construct candidate pairs
Output file: `prompt_responses_pair.json`
"""
import sys
import os
sys.path.append('./')
sys.path.append('../')
sys.path.append('../../')
sys.path.append('../../../')
import argparse
import json
from tqdm import tqdm
from typing import TYPE_CHECKING, List, Optional

from src.alignment.data import PairwiseDataCollatorWithPadding, get_dataset
from src.alignment.extras.ploting import plot_loss
from src.alignment.model import load_model, load_tokenizer
from src.alignment.train.callbacks import fix_valuehead_checkpoint, LogCallback
from src.alignment.train.trainer_utils import create_modelcard_and_push, create_reward_model
from src.alignment.train.rm.metric import ComputeAccuracy
from src.alignment.train.rm.trainer import PairwiseTrainer
from src.alignment.hparams import get_infer_args, get_train_args
from src.alignment.extras.read_yaml_args import read_yaml_file

if TYPE_CHECKING:
    from transformers import Seq2SeqTrainingArguments, TrainerCallback

    from src.alignment.hparams import DataArguments, FinetuningArguments


def load_prompt_response(data_path, cut_num: int=None):
    """
    output:
    [
        {
            "messages": [
                {"role": "system", "content": "xxx"},
                {"role": "user", "content": "xxx"},
            ],
            "reference": {"role": "assistant", "content": "xxx"},
            "from": "xx",
            "responses": [{"role": "assistant", "content": "xx"}, ...],
        }
    ]

    """
    examples = list()
    with open(data_path, "r", encoding="utf-8") as fr:
        for line in tqdm(fr.readlines()):
            example = json.loads(line)
            if type(example["responses"][0]) == str:
                responses = list()
                for response in example["responses"]:
                    responses.append({
                        "role": "assistant",
                        "content": response,
                    })
                example["responses"] = responses
            assert type(example["responses"][0]) == dict, "the response must be dict, like {'role': 'xx', 'content': 'xx'}"
            examples.append(example)
    if cut_num is not None:
        examples = examples[:cut_num]
    return examples

def apply_chat_template(examples, tokenizer, is_tokenize=False, device=None):
    
    """
    output:
    [
        [
            [23, 43, ...],
            [xx, xx, ...]
        ],
        ...
    ]
    """


    all_tokenized_examples = list()
    for example in tqdm(examples):
        messages = example["messages"]
        reference = example["reference"]
        responses = example["responses"]

        all_conversations = list()
        all_tokenized_conversations = list()

        # the first conversation is reference labeled by human, the rest are responses generated by the policy.
        all_conversations.append(messages + [reference])
        for response in responses:
            all_conversations.append(messages + [response])
        
        for conversation in all_conversations:

            tokenized_conversation = tokenizer.apply_chat_template(
                conversation,
                tokenize=False,
                add_generation_prompt=True,
                return_tensor="pt"
            )
            if is_tokenize:
                tokenized_conversation = tokenizer([tokenized_conversation], return_tensors="pt").to(device)
            
            all_tokenized_conversations.append(tokenized_conversation)
        assert len(all_tokenized_conversations) == len(all_conversations)
        all_tokenized_examples.append(all_tokenized_conversations)
    return all_tokenized_examples

def assign_reward(reward_model, prompt_responses, tokenized_prompt_responses):
    """
    return format:
    {
        "messages": [{"role": "xx", "content": "xx"}, xxx],
        "from": "xxx",
        "reference": {"role": "assistant", "content": "xxx"},
        "responses": [{"role": "assistant", "content": "xx"}, ...],
        "reward_scores_of_reference": xx,
        "reward_scores_of_responses": [xx, xx, ..]
    }
    """
    # assign reward scores for each reference and responses of prompt data
    for example, tokenized_example in tqdm(zip(prompt_responses, tokenized_prompt_responses), total=len(prompt_responses)):
        reward_scores = list()
        for tokenized_conversation in tokenized_example:
            _, _, values = reward_model(**tokenized_conversation, return_dict=True, use_cache=False) # []
            rewards = values.gather(dim=-1, index=(tokenized_conversation["attention_mask"].sum(dim=-1, keepdim=True) - 1))
            rewards = rewards.squeeze()
            reward_scores.append(round(rewards.float().detach().cpu().numpy().tolist(), 4))
        assert len(reward_scores) == len(example["responses"]) + 1
        example["reward_scores_of_reference"] = reward_scores[0]
        example["reward_scores_of_responses"] = reward_scores[1:]
    return prompt_responses

if __name__ == "__main__":
    # args_file = sys.argv[1]
    # args = read_yaml_file(args_file)
    # print(args)
    # callbacks = list()
    # callbacks.append(LogCallback())
    parser = argparse.ArgumentParser()
    #type是要传入的参数的数据类型  help是该参数的提示信息
    parser.add_argument("--prompt_responses_path", type=str, help="The path of prompt set")
    parser.add_argument("--iteration_stage", type=int, help="The stage idx of the iteration preference optimization, e.g., 1, 2, 3")
    parser.add_argument("--model_name_or_path", type=str, help="The model name or path of LLM")
    parser.add_argument("--model_name", type=str, help="The model name")
    parser.add_argument("--reward_model", type=str, default=None, help="The lora model path of reward model")
    parser.add_argument("--save_dir", type=str, default="outputs/llm_bench", help="The save path (must equal to the benchmark directory, e.g., llm_bench)")
    parser.add_argument("--cut_n", type=int, default=None, help="The number of examples cut off when debugging")
    parser.add_argument("--block_num", type=int, default=1, help="The number of block")
    parser.add_argument("--local-rank", type=int, default=-1, help="The number of block")
    parser.add_argument('--do_train', action='store_true', help="Whether to train the model")

    # parser.add_argument("--save_step", type=int, default=10, help='保存间隔')
    args = parser.parse_args()
    args_json = {
        "model_name_or_path": args.model_name_or_path, # base backbone model path
        "adapter_name_or_path": args.reward_model, # value head model path
        "reward_model_adapters": args.reward_model,
        "reward_model": args.reward_model, # lora model path
        "output_dir": args.save_dir,
        "template": args.model_name,
        # "reward_model_type": "inference",
    }

    save_path = os.path.join(args.save_dir, f"iteration_stage_{args.iteration_stage}", "prompt_responses_with_rewards/")
    if not os.path.exists(save_path):
        os.makedirs(save_path)

    model_args, data_args, training_args, finetuning_args, generating_args = get_train_args(args_json)

    # load tokenizer
    print("load tokenizer ...")
    tokenizer_module = load_tokenizer(model_args)
    tokenizer = tokenizer_module["tokenizer"]

    # load pre-trained reward model
    print("load trained reward model ...")
    reward_model = load_model(tokenizer, model_args, finetuning_args, training_args.do_train, add_valuehead=True)
    create_reward_model(reward_model, model_args, finetuning_args)
    reward_model.eval()

    for name, param in reward_model.named_parameters():
        print(f"{name}")

    # load prompt reponses generated by the policy at the last iteration stage
    print("load prompt responses generated by the policy ...")
    prompt_responses = load_prompt_response(args.prompt_responses_path, cut_num=args.cut_n)
    tokenized_prompt_responses = apply_chat_template(
        prompt_responses, 
        tokenizer, 
        is_tokenize=True, 
        device=reward_model.pretrained_model.device
    )
    
    print("assign reward for each reference and response ...")
    prompt_responses_with_rewards = assign_reward(reward_model, prompt_responses, tokenized_prompt_responses)

    # save prompt responses with reward scores
    with open(os.path.join(save_path, "prompt_responses_with_rewards.json"), "a", encoding="utf-8") as fw:
        for example in tqdm(prompt_responses_with_rewards):
            fw.write(json.dumps(example, ensure_ascii=False) + "\n")

    # simple evaluation
    # we expect the reward of reference must higher than every response generated by the policy. so that we evaluate the accuracy.
    acc_per_example = 0 # for each prompt, if all scores of responses are lower than reference, then plus 1.
    acc_per_response = 0 # for each prompt each reference-response pair, if the score of response is lower than reference, then plus 1.
    all_prompts = len(prompt_responses_with_rewards) 
    all_pairs = 0
    for example in prompt_responses_with_rewards:
        reward_scores_of_reference = example["reward_scores_of_reference"]
        reward_scores_of_responses = example["reward_scores_of_responses"]
        if reward_scores_of_reference > max(reward_scores_of_responses):
            acc_per_example += 1
        all_pairs += len(reward_scores_of_responses)
        for response_score in reward_scores_of_responses:
            if reward_scores_of_reference > response_score:
                acc_per_response += 1
    print("accuracy per example: {}".format(round(acc_per_example / all_prompts, 4)))
    print("accuracy per response: {}".format(round(acc_per_response / all_pairs, 4)))
    print("done.")

